read_docs:
  desc: how many docs to load, at most
  value: 9018
window_len:
  desc: size of token sequences to train on (and network size!)
  value: 30
pad_windows:
  desc: zero pad beginning and end of doc token stream
  value: 1
amount_feature:
  desc: use dollar value directly as feature?
  value: 1
vocab_size:
  value: 500
token_dims:
  desc: number of features per token, including token hash
  value: 8
positive_fraction:
  desc: target match scores larger than this will becomes positive labels
  value: 0.5
target_thresh:
  value: 0.9
epochs:
  value: 50
batch_size:
  value: 10000
steps_per_epoch:
  desc: how many documents to check extraction on after each epoch
  value: 10
doc_acc_sample_size:
  value: 150
penalize_missed:
  desc: how much more a missed 1 counts than a missed 0 in outputs
  value: 5
val_split:
  value: 0.2
len_train:
  value: 100 #Not more than the value of read_docs
trim_low_scores:
  desc: used to trim away low scores (scores below target_thresh) 
  value: true
use_data_cache:
  desc: use pickled saved training data (freezes options like padding, amount_feature)
  value: 0
data_from_db:
  desc: load from 'db'
  value: false
db_user:
  value: root
db_password:
  value: changeme
