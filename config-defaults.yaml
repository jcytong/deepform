len_train:
  dec: number of documents to use (training + validation)
  value: 8000

val_split:
  value: 0.2

# sweeps suggest these are reasonable hyperparameter defaults
window_len:
  desc: size of token sequences to train on, must be odd
  value: 25

# feature generation
pad_windows:
  desc: zero pad beginning and end of doc token stream
  value: 1
use_amount:
  desc: use token dollar value directly as feature?
  value: 1
use_page:
  desc: use token page number as feature?
  value: 1
use_geom:
  desc: use token geometry (bbox corner) as feature?
  value: 1
use_string:
  desc: use token string as feature?
  value: 1

vocab_size:
  desc: token strings are hashed mod this before embedding
  value: 500
vocab_embed_size:
  desc: number of outputs in token hash embedding
  value: 16

target_thresh:
  desc: throw away token matches to PP crowdsourced data that aren't at least this good
  value: 0.8

# network size
layer_1_size_factor:
  desc: layer 1 size = this factor * window_len
  value: 4
layer_2_size_factor:
  desc: layer 2 size = this factor * window_len
  value: 2
dropout:
  value: 0.2

# training config
epochs:
  value: 50
steps_per_epoch:
  value: 50
batch_size:
  desc: batch size in windows (not docs)
  value: 10000
positive_fraction:
  desc: target match scores larger than this will becomes positive labels
  value: 0.5

penalize_missed:
  desc: how much more a missed 1 counts than a missed 0 in outputs
  value: 5

learning_rate:
  value: 0.001

# These do not affect the training but control various setup and reporting
render_results_size:
  desc: log this many PDF images on last epoch
  value: 20
use_data_cache:
  desc: use pickled saved training data (freezes options like padding, amount_feature)
  value: 1
doc_acc_max_sample_size:
  desc: never sample more than this many documents
  value: 1000
doc_acc_sample_size:
  desc: sample epoch+this documents to compute doc_val_acc (uses all docs on last epoch)
  value: 10
use_wandb:
  desc: report run to wandb and store annotations
  value: 1
log_level:
  desc: minimum level to report in the logs
  value: INFO

# the rest of these are not hyperparamters, but config settings
# meaning it may break things if you change them
token_dims:
  desc: number of features per token, including token hash
  value: 8
